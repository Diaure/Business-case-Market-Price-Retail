## **MINI PROJET ETL / CLOUD COMPUTING : Construction d'un projet Pipeline de Donn√©es de Bout en Bout** <br>
 
![tffgf](./images\Capture.PNG) <br>

Dans le cadre de ce mini-projet, nous nous attaquons √† la construction d‚Äôun pipeline de donn√©es complet en utilisant un jeu de donn√©es brut. L‚Äôobjectif principal est de d√©velopper un processus efficace de transformation de donn√©es pour r√©pondre √† des besoins analytiques sp√©cifiques.

Nous d√©butons par l'examen du sch√©ma physique du jeu de donn√©es initial, n√©cessitant une d√©normalisation pour aboutir √† un sch√©ma adapt√© √† un Data Warehouse.

Le projet met l‚Äôaccent sur l‚Äôutilisation de DBT (Data Build Tool) pour orchestrer les transformations de donn√©es et concevoir le sch√©ma de l‚Äôentrep√¥t de donn√©es, h√©berg√© sur Amazon Redshift. En travaillant avec ces technologies, nous visons √† am√©liorer notre ma√Ætrise de SQL tout en approfondissant notre compr√©hension du data modeling.<br>

## **Objectifs** <br>

üóÇÔ∏è **Mod√©lisation des donn√©es** : Concevoir un sch√©ma physique du jeu de donn√©es brut et le d√©normaliser pour r√©pondre aux besoins analytiques.<br>

‚òÅÔ∏è **Int√©gration des donn√©es dans Redshift** : Mettre en place un entrep√¥t de donn√©es sur Amazon Redshift et cr√©er un cluster Redshift Serverless.<br>

üîß **Transformation des donn√©es avec DBT** : Utiliser DBT pour orchestrer les transformations n√©cessaires et structurer les donn√©es pour l‚Äôanalyse.<br>

üìä **Identification de KPI et requ√™tes analytiques** : D√©finir les indicateurs cl√©s de performance (KPI) et √©crire des requ√™tes SQL pour r√©pondre aux questions analytiques.<br>

## **Technologies utilis√©es**<br>

* **DBT (Data Build Tool)**<br>

* **Amazon Redshift** (avec cluster Serverless)

* **SQL**<br>

Ce projet repr√©sente une opportunit√© d‚Äôappliquer les concepts de mod√©lisation des donn√©es et de renforcer notre expertise en ing√©nierie des donn√©es, tout en veillant √† optimiser notre pipeline pour les besoins analytiques de l‚Äôorganisation. üöÄ<br>

## **Contexte**<br>

**ShopSphere**, une entreprise de vente au d√©tail souhaite mieux comprendre ses op√©rations commerciales, son activit√©, afin de prendre des d√©cisions plus √©clair√©es en mati√®re de gestion des stocks, d'optimisation des prix, et de maximisation des ventes..<br> 

![tffgf](./images\shopshere.PNG) <br>

Pour se faire, elle nous a fournit un jeu de donn√©es qui contient des informations sur les clients, les produits, les commandes, les magasins, et les approvisionnements. Ce jeu de donn√©es permet de suivre plusieurs aspects cl√©s du business, tels que :<br>

* **Suivi des commandes clients** : Comprendre qui ach√®te quoi, quand, et dans quel magasin.<br>

* **Gestion des stocks** : Suivre les produits command√©s et les niveaux d'approvisionnement en fonction des articles vendus et de leur disponibilit√©.<bf>

* **Analyse des performances des magasins** : Mesurer l'efficacit√© des magasins en fonction des commandes trait√©es, des ventes r√©alis√©es, et des taux de taxe appliqu√©s.<br>

* **Optimisation des co√ªts** : √âvaluer les co√ªts des approvisionnements, particuli√®rement ceux des articles p√©rissables, pour mieux g√©rer les marges.<br>

## **Pr√©sentation des donn√©ess**<br>

Nous avons re√ßu du client six tables distinctes qui se d√©finissent ainsi:<bd>

**raw_customers**<br>
* **id** : Identifiant unique du client (integer).<br>
* **name** : Nom du client (string).<br>

**raw_items**<br>
* **id** : Identifiant unique de l'article (integer).<br>
* **order_id** : Identifiant de la commande √† laquelle l'article appartient (integer).<br>
* **sku** : Code de l'unit√© de gestion des stocks (stock keeping unit) pour l'article (string).<br>

**raw_orders**<br>
* **id** : Identifiant unique de la commande (integer).<br>
* **customer_id** : Identifiant du client qui a pass√© la commande (integer).<br>
* **ordered_at** : Date et heure de la commande (timestamp).<br>
* **store_id** : Identifiant du magasin o√π la commande a √©t√© pass√©e (integer).<br>

**raw_products**<br>
* **sku** : Code de l'unit√© de gestion des stocks pour le produit (string).<br>
* **name** : Nom du produit (string).<br>
* **type** : Type de produit (string).<br>
* **price** : Prix du produit (decimal).<br>
* **description** : Description du produit (text).<br>

**raw_stores**
* **id** : Identifiant unique du magasin (integer).<br>
* **name** : Nom du magasin (string).<br>
* **opened_at** : Date et heure d'ouverture du magasin (timestamp).<br>
* **tax_rate** : Taux de taxe applicable au magasin (decimal).<br>

**raw_supplies**
* **id** : Identifiant unique de l'approvisionnement (integer).<br>
* **name** : Nom de l'approvisionnement (string).<br>
* **cost** : Co√ªt de l'approvisionnement (decimal).<br>
* **perishable** : Indique si l'approvisionnement est p√©rissable (boolean).<br>
* **sku** : Code de l'unit√© de gestion des stocks pour l'approvisionnement (string).<br>

## **Etapes cl√©s du projet**<br>

### **1.** **Data Modeling**<br>
#### **1.1** **Analyser le jeu de donn√©es brut.**<br>
* **Pr√©sentations des fichiers sources**<br>
![tffgf](./images\descriptif_tables.png) <br>

* **D√©tection des anomalies**<br>
Pas d'anomalies r√©p√©r√©es mais, nous pr√©voyons de modifier le formatage des dates dans les tables `raw_items`, `raw_orders`, et `raw_stores`(inclusion de l'heure inutilis√©e).<br>

* **Visualisations globales des donn√©es**<br>
**ShopSphere** compte, en tout, **`926 clients`**, et **`6 magasins`**. En explorant les donn√©es brutes re√ßues, nous avons faits diverses observations.<br>

  * *Evolution des commandes*<br>
La majorit√© des ventes sont r√©alis√©es entre les mois de `mars et ao√ªt`.<br>

   ![tffgf](./images\evolution_commandes.png) <br>

   * *Distribution des prix des produits*<br>
Afin de mieux comprendre la strat√©gie de prix de l'entreprise, nous avons r√©alis√©s le visuel c-dessous qui permet de distinguer trois principales fourchettes de prix:<br>

        * `Fourchette de prix **400-650**`<br>
        * `Fourchette de prix **650-790**`<br>
        * `Fourchette de prix **1050-1400**`<br>

    ![tffgf](./images\distribution_prix.png) <br>

   * *Prix de vente vs co√ªt d'achat*<br>
Nous souhaitons observer la comparaison entre les prix de vente et les co√ªts d'approvisionnement afin de mieux √©valuer la rentabilit√© des produits .<br>

    ![tffgf](./images\prix_vente_vs_cout_approv.png) <br>

   * *Nombre de commandes par produits et par cat√©gorie*<br>
La cat√©gorie `beverage` comptabilise `71102` commandes.<br>
La cat√©gorie `jaffle` comptabilise `19081` commandes.<br>

    ![tffgf](./images\nombre_commande_par_categorie.png) <br>

#### **1.2** **Concevoir le sch√©ma physique des donn√©es.**<br>

***Sch√©ma physique de la BDD op√©rationelle*** (r√©alis√© avec dbdiagramme?)

![tffgf](./images\sch√©ma_tables.png) <br>

#### **1.3** **D√©terminer les besoins de d√©normalisation pour cr√©er un sch√©ma adapt√© au DataWareHouse.**<br>

Dans ce projet, la d√©normalisation de la base de donn√©es est cruciale pour simplifier les requ√™tes analytiques et am√©liorer les performances. Voici les principaux aspects de la d√©normalisation et les consid√©rations importantes :

* **Table fact_orders_items**<br>
La table `fact_orders_items` est une table d√©normalis√©e qui combine les informations des commandes et des articles. Elle inclut les √©l√©ments suivants:<br>

* **Commandes et Articles** : Cette table fusionne les d√©tails des commandes et des articles pour faciliter les requ√™tes.<br>

* **Comptage des Produits** : Pour √©viter une relation many-to-many avec la table `dim_products_supplies`, la table des faits doit compter le nombre de fois qu‚Äôun m√™me produit a √©t√© command√© dans le m√™me `order_id`. Cela permet d‚Äô√©viter la duplication des donn√©es et de simplifier les analyses en regroupant les quantit√©s command√©es pour chaque produit dans chaque commande.<br>

* **Table dim_products_supplies**<br>
La table `dim_products_supplies` est √©galement d√©normalis√©e pour int√©grer les informations sur les produits et les approvisionnements :<br>

    * **Combinaison de Produits et Approvisionnements**: La table combine les informations sur les produits et leurs approvisionnements pour √©viter la complexit√© du sch√©ma snowflake.<br>

    * **√âviter le Sch√©ma Snowflake**: En fusionnant les informations sur les produits et les approvisionnements dans une seule table dimensionnelle, nous r√©duisons la complexit√© et √©vitons un sch√©ma snowflake. Cela facilite la gestion des donn√©es et am√©liore les performances des requ√™tes analytiques.<br>

* **Table dim_date**<br>
La table `dim_date` est essentielle pour les analyses temporelles et sera g√©n√©r√©e √† l‚Äôaide du package DBT. Le code pour cr√©er cette table vous sera fourni, ce qui simplifiera la g√©n√©ration de la dimension temporelle avec des champs comme les trimestres, les jours de la semaine, les ann√©es, les mois, etc.<br>

***Sch√©ma la BDD op√©rationelle d√©normalis√©e*** (r√©alis√© avec DBT?)
#int√©grer le sch√©ma#

**2.** **Int√©gration des Donn√©es dans Redshift**<br>

* Mettre en place un entrep√¥t de donn√©es sur Amazon Redshift.<br>
* Cr√©er et configurer un cluster Redshift Serverless.<br>
* Importer les donn√©es brutes dans Redshift.<br>

**3.** **Transformation des Donn√©es, Testing et Documentation √† l'aide de DBT**<br>

* Utiliser DBT pour orchestrer les transformations de donn√©es.<br>
* Cr√©er des mod√®les DBT pour structurer les donn√©es selon le sch√©ma d√©normalis√©.<br>
* Effectuer des tests pour valider les transformations.<br>
* Documenter les transformations et les mod√®les dans DBT.<br>

**4.** **Requ√™tes Analytiques**<br>

* Identifier les indicateurs cl√©s de performance (KPI) pertinents.<br>
* √âcrire des requ√™tes SQL pour extraire des insights analytiques √† partir des donn√©es transform√©es.<br>
* Cr√©er des rapports et des visualisations bas√©s sur les r√©sultats des requ√™tes analytiques √† l'aide de librairies Python au choix.<br>